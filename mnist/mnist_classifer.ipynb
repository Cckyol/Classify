{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../train.csv\")\n",
    "imgs_data = train_data.iloc[:,1:]\n",
    "labels_data = train_data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = range(10)\n",
    "lb = LabelBinarizer().fit(label)\n",
    "length = len(labels_data)\n",
    "label_list = lb.transform(np.array(labels_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_list = []\n",
    "for i in range(length):\n",
    "    img_data = np.array(np.reshape(imgs_data.iloc[i,:],(28,28,1)))\n",
    "    img_list.append(img_data)\n",
    "imgs = np.array(img_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_val,y_train,y_val = train_test_split(imgs,label_list,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train*2 -1\n",
    "y_val = y_val*2 -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BinaryNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.insert(0,os.path.abspath(os.path.join(os.getcwd(),\"./nn_playground/binarynet/\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization, MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from binary_ops import binary_tanh as binary_tanh_op\n",
    "from binary_layers import BinaryDense, BinaryConv2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_tanh(x):\n",
    "    return binary_tanh_op(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 1.\n",
    "kernel_lr_multiplier = 'Glorot'\n",
    "\n",
    "# nn\n",
    "batch_size = 50\n",
    "epochs = 20 \n",
    "channels = 1\n",
    "img_rows = 28 \n",
    "img_cols = 28 \n",
    "filters = 32 \n",
    "kernel_size = (3, 3)\n",
    "pool_size = (2, 2)\n",
    "hidden_units = 128\n",
    "classes = 10\n",
    "use_bias = False\n",
    "\n",
    "# learning rate schedule\n",
    "lr_start = 1e-3\n",
    "lr_end = 1e-4\n",
    "lr_decay = (lr_end / lr_start)**(1. / epochs)\n",
    "\n",
    "# BN\n",
    "epsilon = 1e-6\n",
    "momentum = 0.9\n",
    "\n",
    "# dropout\n",
    "p1 = 0.25\n",
    "p2 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# conv1\n",
    "model.add(BinaryConv2D(128, kernel_size=kernel_size, input_shape=(img_rows, img_cols,channels),\n",
    "                       data_format='channels_last',\n",
    "                       H=H, kernel_lr_multiplier=kernel_lr_multiplier, \n",
    "                       padding='same', use_bias=use_bias, name='conv1'))\n",
    "model.add(BatchNormalization(epsilon=epsilon, momentum=momentum, axis=1, name='bn1'))\n",
    "model.add(Activation(binary_tanh, name='act1'))\n",
    "# conv2\n",
    "model.add(BinaryConv2D(128, kernel_size=kernel_size, H=H, kernel_lr_multiplier=kernel_lr_multiplier, \n",
    "                       data_format='channels_last',\n",
    "                       padding='same', use_bias=use_bias, name='conv2'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size, name='pool2', data_format='channels_last'))\n",
    "model.add(BatchNormalization(epsilon=epsilon, momentum=momentum, axis=1, name='bn2'))\n",
    "model.add(Activation(binary_tanh, name='act2'))\n",
    "# conv3\n",
    "model.add(BinaryConv2D(256, kernel_size=kernel_size, H=H, kernel_lr_multiplier=kernel_lr_multiplier,\n",
    "                       data_format='channels_last',\n",
    "                       padding='same', use_bias=use_bias, name='conv3'))\n",
    "model.add(BatchNormalization(epsilon=epsilon, momentum=momentum, axis=1, name='bn3'))\n",
    "model.add(Activation(binary_tanh, name='act3'))\n",
    "# conv4\n",
    "model.add(BinaryConv2D(256, kernel_size=kernel_size, H=H, kernel_lr_multiplier=kernel_lr_multiplier,\n",
    "                       data_format='channels_last',\n",
    "                       padding='same', use_bias=use_bias, name='conv4'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size, name='pool4', data_format='channels_last'))\n",
    "model.add(BatchNormalization(epsilon=epsilon, momentum=momentum, axis=1, name='bn4'))\n",
    "model.add(Activation(binary_tanh, name='act4'))\n",
    "model.add(Flatten())\n",
    "# dense1\n",
    "model.add(BinaryDense(1024, H=H, kernel_lr_multiplier=kernel_lr_multiplier, use_bias=use_bias, name='dense5'))\n",
    "model.add(BatchNormalization(epsilon=epsilon, momentum=momentum, name='bn5'))\n",
    "model.add(Activation(binary_tanh, name='act5'))\n",
    "# dense2\n",
    "model.add(BinaryDense(classes, H=H, kernel_lr_multiplier=kernel_lr_multiplier, use_bias=use_bias, name='dense6'))\n",
    "model.add(BatchNormalization(epsilon=epsilon, momentum=momentum, name='bn6'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (BinaryConv2D)         (None, 28, 28, 128)       1152      \n",
      "_________________________________________________________________\n",
      "bn1 (BatchNormalization)     (None, 28, 28, 128)       112       \n",
      "_________________________________________________________________\n",
      "act1 (Activation)            (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2 (BinaryConv2D)         (None, 28, 28, 128)       147456    \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "bn2 (BatchNormalization)     (None, 14, 14, 128)       56        \n",
      "_________________________________________________________________\n",
      "act2 (Activation)            (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv3 (BinaryConv2D)         (None, 14, 14, 256)       294912    \n",
      "_________________________________________________________________\n",
      "bn3 (BatchNormalization)     (None, 14, 14, 256)       56        \n",
      "_________________________________________________________________\n",
      "act3 (Activation)            (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv4 (BinaryConv2D)         (None, 14, 14, 256)       589824    \n",
      "_________________________________________________________________\n",
      "pool4 (MaxPooling2D)         (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "bn4 (BatchNormalization)     (None, 7, 7, 256)         28        \n",
      "_________________________________________________________________\n",
      "act4 (Activation)            (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense5 (BinaryDense)         (None, 1024)              12845056  \n",
      "_________________________________________________________________\n",
      "bn5 (BatchNormalization)     (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "act5 (Activation)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense6 (BinaryDense)         (None, 10)                10240     \n",
      "_________________________________________________________________\n",
      "bn6 (BatchNormalization)     (None, 10)                40        \n",
      "=================================================================\n",
      "Total params: 13,893,028\n",
      "Trainable params: 13,890,834\n",
      "Non-trainable params: 2,194\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(lr=lr_start) \n",
    "model.compile(loss='squared_hinge', optimizer=opt, metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/20\n",
      "33600/33600 [==============================] - 22s 647us/step - loss: 0.0944 - acc: 0.9322 - val_loss: 0.0175 - val_acc: 0.9843\n",
      "Epoch 2/20\n",
      "33600/33600 [==============================] - 21s 628us/step - loss: 0.0237 - acc: 0.9812 - val_loss: 0.0158 - val_acc: 0.9850\n",
      "Epoch 3/20\n",
      "33600/33600 [==============================] - 21s 624us/step - loss: 0.0194 - acc: 0.9832 - val_loss: 0.0117 - val_acc: 0.9888\n",
      "Epoch 4/20\n",
      "33600/33600 [==============================] - 21s 627us/step - loss: 0.0188 - acc: 0.9836 - val_loss: 0.0112 - val_acc: 0.9893\n",
      "Epoch 5/20\n",
      "33600/33600 [==============================] - 21s 627us/step - loss: 0.0180 - acc: 0.9840 - val_loss: 0.0106 - val_acc: 0.9889\n",
      "Epoch 6/20\n",
      "33600/33600 [==============================] - 21s 632us/step - loss: 0.0161 - acc: 0.9861 - val_loss: 0.0111 - val_acc: 0.9876\n",
      "Epoch 7/20\n",
      "33600/33600 [==============================] - 21s 635us/step - loss: 0.0159 - acc: 0.9862 - val_loss: 0.0116 - val_acc: 0.9871\n",
      "Epoch 8/20\n",
      "33600/33600 [==============================] - 21s 628us/step - loss: 0.0157 - acc: 0.9865 - val_loss: 0.0099 - val_acc: 0.9890\n",
      "Epoch 9/20\n",
      "33600/33600 [==============================] - 21s 634us/step - loss: 0.0149 - acc: 0.9878 - val_loss: 0.0097 - val_acc: 0.9895\n",
      "Epoch 10/20\n",
      "33600/33600 [==============================] - 21s 634us/step - loss: 0.0137 - acc: 0.9896 - val_loss: 0.0097 - val_acc: 0.9896\n",
      "Epoch 11/20\n",
      "33600/33600 [==============================] - 21s 630us/step - loss: 0.0139 - acc: 0.9886 - val_loss: 0.0083 - val_acc: 0.9911\n",
      "Epoch 12/20\n",
      "33600/33600 [==============================] - 21s 626us/step - loss: 0.0134 - acc: 0.9888 - val_loss: 0.0090 - val_acc: 0.9902\n",
      "Epoch 13/20\n",
      "33600/33600 [==============================] - 21s 633us/step - loss: 0.0130 - acc: 0.9900 - val_loss: 0.0095 - val_acc: 0.9895\n",
      "Epoch 14/20\n",
      "33600/33600 [==============================] - 21s 630us/step - loss: 0.0123 - acc: 0.9906 - val_loss: 0.0088 - val_acc: 0.9904\n",
      "Epoch 15/20\n",
      "33600/33600 [==============================] - 21s 632us/step - loss: 0.0119 - acc: 0.9901 - val_loss: 0.0083 - val_acc: 0.9904\n",
      "Epoch 16/20\n",
      "33600/33600 [==============================] - 21s 634us/step - loss: 0.0124 - acc: 0.9904 - val_loss: 0.0086 - val_acc: 0.9910\n",
      "Epoch 17/20\n",
      "33600/33600 [==============================] - 21s 634us/step - loss: 0.0114 - acc: 0.9918 - val_loss: 0.0083 - val_acc: 0.9913\n",
      "Epoch 18/20\n",
      "33600/33600 [==============================] - 21s 629us/step - loss: 0.0112 - acc: 0.9918 - val_loss: 0.0081 - val_acc: 0.9912\n",
      "Epoch 19/20\n",
      "33600/33600 [==============================] - 21s 630us/step - loss: 0.0120 - acc: 0.9913 - val_loss: 0.0088 - val_acc: 0.9895\n",
      "Epoch 20/20\n",
      "33600/33600 [==============================] - 21s 627us/step - loss: 0.0117 - acc: 0.9915 - val_loss: 0.0080 - val_acc: 0.9906\n"
     ]
    }
   ],
   "source": [
    "# lr_scheduler = LearningRateScheduler(lambda e: lr_start * lr_decay ** e)\n",
    "lr_scheduler = LearningRateScheduler(lambda x: 1e-3 * 0.9 **x)\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=1, validation_data=(x_val, y_val),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenya/.pyenv/versions/2.7.12/envs/env2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import  Sequential,Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D,Dense,MaxPooling2D,Flatten,InputLayer,Input\n",
    "from keras.layers import BatchNormalization, Dropout,Activation\n",
    "from keras.activations import  relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "?? K.stop_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "classfier_model = Sequential()\n",
    "classfier_model.add(InputLayer(input_shape=(28,28,1),name=\"Input\"))\n",
    "classfier_model.add(BatchNormalization(axis=3))\n",
    "classfier_model.add(Conv2D(16,(3,3),padding=\"same\",name=\"block1_conv1\"))\n",
    "classfier_model.add(Conv2D(16,(3,3),padding=\"same\",activation=\"relu\",name=\"block1_con2\"))\n",
    "classfier_model.add(MaxPooling2D((2, 2), strides=(2, 2), name='pool1'))\n",
    "classfier_model.add(Conv2D(48,(3,3),padding=\"same\",name=\"block2_conv1\"))\n",
    "classfier_model.add(Conv2D(48,(3,3),padding=\"same\",activation=\"relu\",name=\"block2_conv2\"))\n",
    "classfier_model.add(MaxPooling2D((2, 2), strides=(2, 2), name='pool2'))\n",
    "classfier_model.add(Flatten(name=\"flatten\"))\n",
    "classfier_model.add(Dense(512, activation='relu', name='fc1'))\n",
    "classfier_model.add(Dense(10,activation=\"softmax\",name=\"prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classfier_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-ef9bc05d9e55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0madam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclassfier_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mclassfier_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classfier_model' is not defined"
     ]
    }
   ],
   "source": [
    "adam = Adam(lr=0.01)\n",
    "classfier_model.compile(optimizer=adam, loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "classfier_model.fit(x_train,y_train,validation_data=(x_val,y_val),epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(28,28,1))\n",
    "x = BatchNormalization(axis=3)\n",
    "x = Conv2D(20,(5,5),padding=\"valid\")(inputs)\n",
    "x = MaxPooling2D((2,2),strides=(2,2))(x)\n",
    "x = Conv2D(50,(5,5),padding=\"valid\")(x)\n",
    "x = MaxPooling2D((2,2),strides=(2,2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(60, activation=\"relu\")(x)\n",
    "x = Dense(48, activation=\"relu\")(x)\n",
    "predictions = Dense(10, activation=\"softmax\")(x)\n",
    "model = Model(inputs=inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/20\n",
      " 8416/33600 [======>.......................] - ETA: 22s - loss: -22.5953 - acc: 0.4414"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-bfcefd4a2101>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/chenya/.pyenv/versions/2.7.12/envs/env2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/chenya/.pyenv/versions/2.7.12/envs/env2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/chenya/.pyenv/versions/2.7.12/envs/env2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/chenya/.pyenv/versions/2.7.12/envs/env2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/chenya/.pyenv/versions/2.7.12/envs/env2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/chenya/.pyenv/versions/2.7.12/envs/env2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/chenya/.pyenv/versions/2.7.12/envs/env2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/chenya/.pyenv/versions/2.7.12/envs/env2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/chenya/.pyenv/versions/2.7.12/envs/env2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "model.fit(x_train,y_train,validation_data=(x_val,y_val),verbose=1,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"mnist_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/20\n",
      "33312/33600 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.9837\n",
      "Epoch 00001: val_loss improved from inf to 0.09565, saving model to mnist_model.h5\n",
      "33600/33600 [==============================] - 4s 110us/step - loss: 0.0907 - acc: 0.9838 - val_loss: 0.0957 - val_acc: 0.9839\n",
      "Epoch 2/20\n",
      "33408/33600 [============================>.] - ETA: 0s - loss: 0.0826 - acc: 0.9870\n",
      "Epoch 00002: val_loss did not improve\n",
      "33600/33600 [==============================] - 3s 104us/step - loss: 0.0823 - acc: 0.9870 - val_loss: 0.1181 - val_acc: 0.9811\n",
      "Epoch 3/20\n",
      "33280/33600 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9911\n",
      "Epoch 00003: val_loss did not improve\n",
      "33600/33600 [==============================] - 4s 105us/step - loss: 0.0484 - acc: 0.9911 - val_loss: 0.0993 - val_acc: 0.9849\n",
      "Epoch 4/20\n",
      "33504/33600 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9946\n",
      "Epoch 00004: val_loss did not improve\n",
      "33600/33600 [==============================] - 4s 110us/step - loss: 0.0299 - acc: 0.9946 - val_loss: 0.1075 - val_acc: 0.9833\n",
      "Epoch 5/20\n",
      "33376/33600 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9948\n",
      "Epoch 00005: val_loss did not improve\n",
      "33600/33600 [==============================] - 4s 106us/step - loss: 0.0325 - acc: 0.9948 - val_loss: 0.1151 - val_acc: 0.9829\n",
      "Epoch 6/20\n",
      "33344/33600 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9945\n",
      "Epoch 00006: val_loss did not improve\n",
      "33600/33600 [==============================] - 3s 104us/step - loss: 0.0340 - acc: 0.9944 - val_loss: 0.1068 - val_acc: 0.9818\n",
      "Epoch 7/20\n",
      "33536/33600 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9964\n",
      "Epoch 00007: val_loss improved from 0.09565 to 0.09252, saving model to mnist_model.h5\n",
      "33600/33600 [==============================] - 4s 106us/step - loss: 0.0177 - acc: 0.9964 - val_loss: 0.0925 - val_acc: 0.9854\n",
      "Epoch 8/20\n",
      "33504/33600 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9974\n",
      "Epoch 00008: val_loss did not improve\n",
      "33600/33600 [==============================] - 4s 105us/step - loss: 0.0119 - acc: 0.9974 - val_loss: 0.0935 - val_acc: 0.9851\n",
      "Epoch 9/20\n",
      "33216/33600 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9976\n",
      "Epoch 00009: val_loss did not improve\n",
      "33600/33600 [==============================] - 4s 106us/step - loss: 0.0137 - acc: 0.9976 - val_loss: 0.1121 - val_acc: 0.9823\n",
      "Epoch 10/20\n",
      "33280/33600 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9983\n",
      "Epoch 00010: val_loss did not improve\n",
      "33600/33600 [==============================] - 4s 105us/step - loss: 0.0089 - acc: 0.9983 - val_loss: 0.0997 - val_acc: 0.9855\n",
      "Epoch 11/20\n",
      "33472/33600 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9981\n",
      "Epoch 00011: val_loss did not improve\n",
      "33600/33600 [==============================] - 4s 106us/step - loss: 0.0089 - acc: 0.9980 - val_loss: 0.1032 - val_acc: 0.9860\n",
      "Epoch 12/20\n",
      "33216/33600 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9986\n",
      "Epoch 00012: val_loss improved from 0.09252 to 0.09172, saving model to mnist_model.h5\n",
      "33600/33600 [==============================] - 4s 106us/step - loss: 0.0061 - acc: 0.9987 - val_loss: 0.0917 - val_acc: 0.9874\n",
      "Epoch 13/20\n",
      "33376/33600 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9984\n",
      "Epoch 00013: val_loss did not improve\n",
      "33600/33600 [==============================] - 4s 108us/step - loss: 0.0086 - acc: 0.9984 - val_loss: 0.1090 - val_acc: 0.9864\n",
      "Epoch 14/20\n",
      "33056/33600 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9988\n",
      "Epoch 00014: val_loss did not improve\n",
      "33600/33600 [==============================] - 4s 105us/step - loss: 0.0057 - acc: 0.9988 - val_loss: 0.1001 - val_acc: 0.9867\n",
      "Epoch 15/20\n",
      "33376/33600 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9989\n",
      "Epoch 00015: val_loss did not improve\n",
      "33600/33600 [==============================] - 3s 104us/step - loss: 0.0045 - acc: 0.9989 - val_loss: 0.0978 - val_acc: 0.9871\n",
      "Epoch 16/20\n",
      "33120/33600 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9988\n",
      "Epoch 00016: val_loss did not improve\n",
      "33600/33600 [==============================] - 4s 106us/step - loss: 0.0050 - acc: 0.9988 - val_loss: 0.0962 - val_acc: 0.9879\n",
      "Epoch 17/20\n",
      "33504/33600 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9990\n",
      "Epoch 00017: val_loss did not improve\n",
      "33600/33600 [==============================] - 4s 110us/step - loss: 0.0050 - acc: 0.9990 - val_loss: 0.0960 - val_acc: 0.9882\n",
      "Epoch 18/20\n",
      "33408/33600 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9990\n",
      "Epoch 00018: val_loss did not improve\n",
      "33600/33600 [==============================] - 4s 105us/step - loss: 0.0043 - acc: 0.9990 - val_loss: 0.0925 - val_acc: 0.9886\n",
      "Epoch 19/20\n",
      "33248/33600 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9990\n",
      "Epoch 00019: val_loss did not improve\n",
      "33600/33600 [==============================] - 4s 110us/step - loss: 0.0040 - acc: 0.9990 - val_loss: 0.0963 - val_acc: 0.9880\n",
      "Epoch 20/20\n",
      "33376/33600 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9990\n",
      "Epoch 00020: val_loss did not improve\n",
      "33600/33600 [==============================] - 4s 104us/step - loss: 0.0040 - acc: 0.9990 - val_loss: 0.1040 - val_acc: 0.9871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa2c07daf90>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import LearningRateScheduler,EarlyStopping,ModelCheckpoint\n",
    "batch_size = 32\n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.9 **x)\n",
    "earlystop = EarlyStopping(patience=10)\n",
    "model_save = ModelCheckpoint(\n",
    "                filepath=\"mnist_model.h5\", save_best_only=True, verbose=1\n",
    ")\n",
    "model.fit(x_train,y_train,batch_size=batch_size,\n",
    "          epochs=20, validation_data=(x_val,y_val),\n",
    "          callbacks=[annealer,earlystop,model_save]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../test.csv')\n",
    "test_list = []\n",
    "for i in range(test_data.shape[0]):\n",
    "    test = np.array(np.reshape(test_data.iloc[i,:],(28,28,1)))\n",
    "    test_list.append(test)\n",
    "tests_data = np.array(test_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId  Label\n",
       "0        1      2\n",
       "1        2      0\n",
       "2        3      9\n",
       "3        4      0\n",
       "4        5      3"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = model.predict(tests_data)\n",
    "predict_labels = lb.inverse_transform(predict)\n",
    "values_data = pd.DataFrame({\"ImageId\":np.arange(1,28001),\"Label\":predict_labels})\n",
    "values_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_data.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
